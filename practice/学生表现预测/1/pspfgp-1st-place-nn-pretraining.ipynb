{"metadata":{"kernelspec":{"display_name":"predict-student-performance-from-game-play","language":"python","name":"predict-student-performance-from-game-play"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"papermill":{"default_parameters":{},"duration":242.173644,"end_time":"2023-02-08T17:40:14.115682","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-02-08T17:36:11.942038","version":"2.3.4"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"VERSION = 'pspfgp-48-nn-pretrain'\nFOLD = 2\nDEV = False\nDATA = 'ALL'  # COMP, ALL\nN_BAGS = 1\nN_FOLDS = 5\nN_SEEDS = 1\nD_MODEL = 24\nBATCH_SIZE = 128\nEXPIT = True\nSEED = 0\nGPU = 0\nN_THREADS = 8\nVERBOSE = True\nLEVEL_GROUPS = ['0-4', '5-12', '13-22']\nPREV_LEVEL_GROUPS = False\nLENGTHS = {'0-4': 600, '5-12': 1400, '13-22': 2000}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DISCRETE_FEATURES = ['room_fqid', 'event_name_name', 'text', 'fqid']\nCONTINUOUS_FEATURES = ['duration']\nFEATURES = DISCRETE_FEATURES + CONTINUOUS_FEATURES","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, HTML\ndisplay(HTML('<style>td{white-space: nowrap !important;}</style>'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['CUDA_VISIBLE_DEVICES'] = str(GPU)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport time","metadata":{"execution":{"iopub.execute_input":"2023-02-08T17:36:20.238682Z","iopub.status.busy":"2023-02-08T17:36:20.237962Z","iopub.status.idle":"2023-02-08T17:36:21.293321Z","shell.execute_reply":"2023-02-08T17:36:21.292411Z"},"papermill":{"duration":1.065201,"end_time":"2023-02-08T17:36:21.296054","exception":false,"start_time":"2023-02-08T17:36:20.230853","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if EXPIT:\n    from scipy.special import expit, logit","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.get_logger().setLevel('WARNING')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nos.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'  # TF will not use all memory","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.max_rows\", 250)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def get_questions(level_group):\n    return ([1, 2, 3] if level_group == '0-4' \n            else [4, 5, 6, 7, 8, 9, 10, 11, 12, 13] if level_group == '5-12' \n            else [14, 15, 16, 17, 18])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def optimize_threshold(preds, labels, step_size=0.001):\n    thresholds = []\n    best_score = 0\n    best_threshold = 0\n    for threshold in np.arange(0, 1, step_size):\n        binarized_preds = (preds.melt().drop('variable', axis=1).values > threshold).astype('int')\n        score = fast_f1_score(labels.melt().drop('variable', axis=1).values, binarized_preds)\n        thresholds.append(threshold)\n        if score > best_score:\n            best_score = score\n            best_threshold = np.round(threshold, 3)\n    return best_threshold","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fast_f1_score(labels, preds):\n    all_positives = (preds + labels == 2).mean()\n    all_negatives = (preds + labels == 0).mean()\n    score = 1 - (1 - all_negatives - all_positives) / (1 - (all_negatives - all_positives) ** 2)\n    return score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_questions(preds, labels, questions, thr):\n    thresholds = thr if type(thr) == list else [thr] * 18\n    scores = dict()\n    preds_binarized = preds.copy()\n    for question in questions:\n        preds_binarized[f'q{question}'] = (preds[f'q{question}'].values > thresholds[question - 1]).astype('int')\n        score = fast_f1_score(labels[f'q{question}'].values, preds_binarized[f'q{question}'].values)\n        scores[f'q{question}'] = np.round(score, decimals=5)\n    if len(questions) > 1:\n        score = fast_f1_score(labels.melt().drop('variable', axis=1).values, \n                              preds_binarized.melt().drop('variable', axis=1).values)\n        scores['overall'] = np.round(score, decimals=5)\n    return scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(df, feats):\n    df_out = df.copy()\n    tokenizer_map = {}\n    for f in feats:\n        factorized = pd.factorize(df[f])\n        df_out[f] = factorized[0] + 1\n        tokenizer_map[f] = {\n            'encode': ['<PAD>'] + list(factorized[1]),\n            'decode': {(i + 1): el for i, el in enumerate(factorized[1])}\n        }\n    return df_out, tokenizer_map","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_score(y_pred, y_true, threshold=None):\n    cols = [f'q{q}' for q in range(1, 18 + 1)]\n    df = pd.DataFrame(index=y_true.index)\n    df.loc[:, cols] = np.hstack([y_pred[level_group] for level_group in LEVEL_GROUPS])\n    if threshold is None:\n        threshold = optimize_threshold(df, y_true)\n    scores = score_questions(y_true, 1 * (df > threshold), range(1, 18 + 1), threshold)\n    scores['thr'] = threshold\n    return scores, df, threshold","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_sequence(df, n_features, length):\n    return (np.vstack([df.values, np.zeros((length - len(df), n_features))]) \n            if len(df) < length else df[:length].values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_targets(dataset):\n    targets = []\n    for level_group in LEVEL_GROUPS:\n        t = [b[1][level_group] for b in list(dataset)]\n        targets.append(np.vstack(t))\n    targets = np.hstack(targets)\n    return targets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_dataset(data, session_ids):\n    x_tmp = np.array([data[s][0] for s in session_ids])\n    x = {f: x_tmp[:, :, i] for i, f in enumerate(FEATURES)}\n    y = np.array([data[s][1] for s in session_ids])\n    return x, y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"%%time\nMETADATA = pickle.load(open(f'../final/METADATA.pkl', 'rb'))\ndataset = pickle.load(open(f'../data/processed/dataset.pkl', 'rb'))\ntokenizer_map = pickle.load(open(f'../data/processed/tokenizer_map.pkl', 'rb'))\npretrain_datasets = pickle.load(open(f'../data/processed/pretrain_datasets.pkl', 'rb'))\nY = pickle.load(open(f'../data/processed/Y.pkl', 'rb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Architecture","metadata":{}},{"cell_type":"code","source":"class ConvBlock(tf.keras.layers.Layer):\n    def __init__(self, d_model, dropout_rate):\n        super(ConvBlock, self).__init__()\n        self.conv1d = tf.keras.layers.Conv1D(d_model, kernel_size=5, padding='same', activation='gelu')\n        self.layer_norm = tf.keras.layers.LayerNormalization()\n        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n        \n    def call(self, inputs):\n        x = self.conv1d(inputs)\n        x = x + inputs\n        x = self.layer_norm(x)\n        outputs = self.dropout(x)\n        return outputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeEmbedding(tf.keras.layers.Layer):\n    def __init__(self, n_blocks, d_model, dropout_rate):\n        super(TimeEmbedding, self).__init__()\n        self.conv_blocks = [ConvBlock(d_model, dropout_rate=dropout_rate) for _ in range(n_blocks)]\n        \n    def call(self, inputs):\n        x = tf.expand_dims(inputs, axis=-1)\n        for conv_block in self.conv_blocks:\n            x = conv_block(x)        \n        return x    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvNet(tf.keras.Model):\n    def __init__(self, input_dims, n_outputs, d_model, n_blocks=4, name=None):\n        super(ConvNet, self).__init__(name=name)\n        self.input_dims = input_dims\n        self.n_outputs = n_outputs\n        self.d_model = d_model\n        self.n_blocks = n_blocks\n        self.event_embedding = tf.keras.layers.Embedding(input_dims['event_name_name'], d_model, mask_zero=True)\n        self.room_embedding = tf.keras.layers.Embedding(input_dims['room_fqid'], d_model, mask_zero=True)\n        self.text_embedding = tf.keras.layers.Embedding(input_dims['text'], d_model, mask_zero=True)\n        self.fqid_embedding = tf.keras.layers.Embedding(input_dims['fqid'], d_model, mask_zero=True)\n        self.duration_embedding = TimeEmbedding(n_blocks=n_blocks, d_model=d_model, dropout_rate=0.2)\n        self.gap = tf.keras.layers.GlobalAveragePooling1D()\n        \n    def call(self, inputs):\n        event = self.event_embedding(inputs['event_name_name'])\n        room = self.room_embedding(inputs['room_fqid'])\n        text = self.text_embedding(inputs['text'])\n        fqid = self.fqid_embedding(inputs['fqid'])\n        duration = self.duration_embedding(inputs['duration'])\n        x = duration * (event + room + text + fqid)\n        outputs = self.gap(x)\n        return outputs\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'input_dims': self.input_dims,\n            'n_outputs': self.n_outputs,\n            'd_model': self.d_model,\n            'n_blocks': self.n_blocks,\n            'name': self._name,\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimpleHead(tf.keras.Model):\n    def __init__(self, n_units, n_outputs, name=None):\n        super(SimpleHead, self).__init__(name=name)\n        self.ffs = [tf.keras.layers.Dense(units, activation='gelu') for units in n_units]\n        self.out = tf.keras.layers.Dense(n_outputs, activation='sigmoid')\n        \n    def call(self, inputs):\n        x = inputs\n        for ff in self.ffs:\n            x = ff(x)\n        outputs = self.out(x)\n        return outputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_convnet(input_dims, level_group, bag, fold):\n    name = f'convnet_{level_group.replace(\"-\", \"_\")}_b{bag}f{fold}'\n    n_outputs = len(get_questions(level_group))\n    convnet = ConvNet(input_dims[level_group], n_outputs, name=name)\n    return convnet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_head(level_group, bag, fold):\n    name = f'head_{level_group.replace(\"-\", \"_\")}_b{bag}f{fold}'\n    n_outputs = len(get_questions(level_group))\n    head = SimpleHead(n_units=[512, 128, 32], n_outputs=n_outputs, name=name)\n    return head","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(convnets, heads, trainable=False):\n    inputs = {}\n    for level_group in LEVEL_GROUPS:\n        inputs[level_group] = {}\n        for feature in FEATURES:\n            name = f'input_{feature}_{level_group.replace(\"-\", \"_\")}'\n            inputs[level_group][feature] = tf.keras.Input(shape=(LENGTHS[level_group]), name=name) \n    \n    for level_group in LEVEL_GROUPS:\n        convnets[level_group].trainable = trainable\n\n    convnet_outputs = {level_group: convnets[level_group](inputs[level_group]) for level_group in LEVEL_GROUPS}\n\n    outputs = {}\n    outputs['0-4'] = heads['0-4'](convnet_outputs['0-4'])\n    outputs['5-12'] = heads['5-12'](\n        tf.keras.layers.Concatenate(name='concat_5_12')(\n            [convnet_outputs['0-4'], convnet_outputs['5-12']]))\n    outputs['13-22'] = heads['13-22'](\n        tf.keras.layers.Concatenate(name='concat_13_22')(\n            [convnet_outputs['0-4'], convnet_outputs['5-12'], convnet_outputs['13-22']]))\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='pspfgp_model')\n    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PreTrainingModel(tf.keras.Model):\n    def __init__(self, convnet, head):\n        super().__init__()\n        self.convnet = convnet\n        self.head = head\n        \n    def call(self, inputs):\n        x = self.convnet(inputs)\n        outputs = self.head(x)\n        return outputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build","metadata":{}},{"cell_type":"code","source":"input_dims = {level_group: (dataset[DISCRETE_FEATURES].max() + 1).T.to_dict() for level_group in LEVEL_GROUPS}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dims","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrain","metadata":{}},{"cell_type":"code","source":"class LogCallback(tf.keras.callbacks.Callback):\n    def __init__(self):\n        self.history = []\n        self.t_0 = time.time()\n\n    def on_epoch_end(self, epoch, logs=None):\n        self.history.append({\n            'time': int(time.time() - self.t_0),\n            'epoch': epoch + 1,\n            'lr': logs['lr'],\n            'loss': logs['loss'],\n            'val_loss': logs['val_loss']\n        })\n        \n        if epoch == 0:\n            print('{:<11}{:<9}{:<8}{:<10}{:<10}'.format('Time', 'Epoch', 'LR', 'Loss', 'Val loss'))\n            \n        best_loss = min([h['val_loss'] for h in self.history])\n        info = self.history[-1]\n        hours = str(info['time'] // 3600).zfill(2)\n        minutes = str(info['time'] // 60 % 60).zfill(2)\n        seconds = str(info['time'] % 60).zfill(2)\n        print('{:<11}{:<9}{:<8}{:<10}{:<10}'.format(\n            '{}:{}:{}'.format(hours, minutes, seconds),\n            f'{epoch + 1}',\n            f\"{round(info['lr'], 4):06.4f}\",\n            f\"{round(info['loss'], 5):07.5f}\",\n            f\"{round(info['val_loss'], 5):07.5f}\" + ('-' if info['val_loss'] == best_loss else ''),\n        ))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def schedule_lr(epoch):\n    if epoch < 20:\n        return 1e-3\n    elif epoch < 30:\n        return 5e-4\n    elif epoch < 40:\n        return 2.5e-4\n    else:\n        return 1e-4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LearningRateSchedulerCallback(tf.keras.callbacks.Callback):\n    def __init__(self):\n        self.best = 2 ** 31\n        self.n_steps_since_last_best = 0\n        self.n_steps_of_best = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        if logs['val_loss'] < self.best:\n            self.best = logs['val_loss']\n            self.n_steps_since_last_best = 0\n            self.n_steps_of_best += 1\n        else:\n            self.n_steps_since_last_best += 1\n            self.n_steps_of_best = 0\n        lr_backup = lr = self.model.optimizer.lr.read_value()\n        lr = lr + 1e-4 if self.n_steps_of_best > 2 else lr - 1e-4 if self.n_steps_since_last_best > 2 else lr\n        lr = 1e-3 if lr > 1e-3 else 1e-4 if lr < 1e-4 else lr\n        logs['lr'] = np.round(lr_backup, 4)\n        self.model.optimizer.lr.assign(lr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping_round_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0,\n    patience=10,\n    verbose=0,\n    mode='auto',\n    baseline=None,\n    restore_best_weights=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nrandom.seed(SEED)\ntf.random.set_seed(SEED)\n\nmodels = {level_group: {} for level_group in LEVEL_GROUPS}\nsession_ids = METADATA[METADATA['fold'] > -1]['session_id'].drop_duplicates().tolist()\noof_preds = pd.DataFrame(index=session_ids, \n                         data=np.zeros((len(session_ids), 18)), \n                         columns=[f'q{q}' for q in range(1, 18 + 1)])\nlabels = Y[Y['session_id'].isin(session_ids)].reset_index(drop=True)\nlogs = []\n\nfor b in range(N_BAGS):\n#     for f in range(N_FOLDS):\n    for f in [FOLD]:\n        print(f'> Bag {b} Fold {f}')\n        print()\n        \n        (x_train, y_train), (x_val, y_val) = pretrain_datasets[f'f{f}']\n        \n        for level_group in LEVEL_GROUPS:\n            print(f'> Pretrain level_group {level_group}')\n            print()\n        \n            metadata = METADATA[(METADATA['fold'] == f) & METADATA[level_group]]\n            val_session_ids = metadata['session_id'].drop_duplicates().tolist()\n            \n            train_dataset = tf.data.Dataset.from_tensor_slices((x_train[level_group], y_train[level_group]))\n            train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n            val_dataset = tf.data.Dataset.from_tensor_slices((x_val[level_group], y_val[level_group]))\n            val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n            \n            n_questions = len(get_questions(level_group))\n            suffix = f'{level_group.replace(\"-\", \"_\")}_b{b}f{f}'\n            convnet = ConvNet(input_dims[level_group], n_questions, name=f'convnet_{suffix}', d_model=D_MODEL)\n            head = SimpleHead(n_units=[512, 128, 64], n_outputs=n_questions, name=f'pretrained_head_{suffix}')\n            \n            pretraining_model = PreTrainingModel(convnet, head)\n            optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n            loss = tf.keras.losses.BinaryCrossentropy()\n            pretraining_model.compile(loss=loss, optimizer=optimizer)\n            lr_callback = LearningRateSchedulerCallback()\n            callbacks = [lr_callback, LogCallback(), early_stopping_round_callback]\n            history = pretraining_model.fit(\n                train_dataset, validation_data=val_dataset, epochs=100, verbose=False, callbacks=callbacks)\n            \n            models[level_group][f'b{b}f{f}'] = {}\n            models[level_group][f'b{b}f{f}']['convnet'] = pretraining_model.convnet\n            models[level_group][f'b{b}f{f}']['pretrained_head'] = pretraining_model.head\n            \n            preds = pretraining_model.predict(val_dataset, verbose=False)\n            for i, q in enumerate(get_questions(level_group)):\n                oof_preds.loc[val_session_ids, f'q{q}'] = preds[:, i]\n                \n            print()\n            \n        y_pred = oof_preds.loc[val_session_ids]\n        y_true = labels.set_index('session_id').loc[val_session_ids]\n        threshold = optimize_threshold(y_pred, y_true)\n        y_bin = 1 * (oof_preds.loc[val_session_ids] > threshold)\n        scores = score_questions(y_true, y_bin, range(1, 18 + 1), threshold)\n        scores['thr'] = threshold\n        for q in range(1, 18 + 1):\n            scores[f'q{q}'] = np.round(scores[f'q{q}'], 3)\n        df = pd.DataFrame([scores])\n        display(df)\n        print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = oof_preds[(oof_preds == 0).sum(axis=1) == 0]\nt = Y.set_index('session_id').loc[p.index]\nprint(score_questions(p, t, questions=range(1, 18 + 1), thr=0.625)['overall'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Export","metadata":{}},{"cell_type":"code","source":"VERSION","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! ls ../models/\"$VERSION\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if FOLD == 0:\n    ! rm -rf ../models/\"$VERSION\"\n    ! mkdir ../models/\"$VERSION\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ls -lh ../models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntf.get_logger().setLevel('ERROR')\n\nfor i, level_group in enumerate(LEVEL_GROUPS):\n    for b in range(N_BAGS):\n#         for f in range(N_FOLDS):\n        for f in [FOLD]:\n            inputs = {}\n            suffix = level_group.replace(\"-\", \"_\")\n            for feature in FEATURES:\n                inputs[feature] = tf.keras.Input(shape=(LENGTHS[level_group]), name=f'input_{feature}_{suffix}') \n            outputs = models[level_group][f'b{b}f{f}']['convnet'](inputs)\n            convnet = tf.keras.Model(inputs=inputs, outputs=outputs, name=f'convnet_{suffix}')\n            convnet.save(f'../models/{VERSION}/convnet_{suffix}_b{b}f{f}.h5')\n\ntf.get_logger().setLevel('WARNING')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(tokenizer_map, open(f'../models/{VERSION}/tokenizer_map.pkl', 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(oof_preds, open(f'../models/{VERSION}/oof_preds_pretrain.pkl', 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls ../models/\"$VERSION\"","metadata":{},"execution_count":null,"outputs":[]}]}