{"metadata":{"kernelspec":{"display_name":"predict-student-performance-from-game-play","language":"python","name":"predict-student-performance-from-game-play"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"papermill":{"default_parameters":{},"duration":242.173644,"end_time":"2023-02-08T17:40:14.115682","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-02-08T17:36:11.942038","version":"2.3.4"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"VERSION = 'pspfgp-47-xgb'\nDEV = False\nMODEL_TYPE = 'XGB'  # XGB, LGB, CATBOOST\nDATA = 'ALL'  # COMP, COMPLETE_SESSIONS, ALL\nBUILD = True\nN_BAGS = 10\nN_FOLDS = 5\nN_SEEDS = 1\nEXPIT = True\nSEED = 0\nGPU = 1\nN_THREADS = 8\nVERBOSE = True\nLEVEL_GROUPS = ['0-4', '5-12', '13-22']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, HTML\ndisplay(HTML('<style>td{white-space: nowrap !important;}</style>'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['CUDA_VISIBLE_DEVICES'] = str(GPU)\nos.environ['POLARS_MAX_THREADS'] = str(N_THREADS)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport itertools\nimport pickle\nimport re\nimport time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport sklearn.neighbors, sklearn.metrics, sklearn.preprocessing\nfrom tqdm.notebook import tqdm\nfrom xgboost import XGBClassifier","metadata":{"execution":{"iopub.execute_input":"2023-02-08T17:36:20.238682Z","iopub.status.busy":"2023-02-08T17:36:20.237962Z","iopub.status.idle":"2023-02-08T17:36:21.293321Z","shell.execute_reply":"2023-02-08T17:36:21.292411Z"},"papermill":{"duration":1.065201,"end_time":"2023-02-08T17:36:21.296054","exception":false,"start_time":"2023-02-08T17:36:20.230853","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if EXPIT:\n    from scipy.special import expit, logit","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"x_test = pd.read_parquet('../data/processed/x_test.parquet')\ny_test = pd.read_parquet('../data/processed/y_test.parquet')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX = pd.concat([pd.read_parquet('../data/processed/x.parquet'), x_test]).reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.sort_values(['session_id', 'index']).reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels = pd.read_parquet('../data/processed/y.parquet')\n\ntrain_labels['question'] = train_labels.session_id.apply(lambda x: x.split('_')[1]).values\ntrain_labels['session_id'] = train_labels.session_id.apply(lambda x: x.split('_')[0]).values\ntrain_labels['session_id'] = train_labels['session_id'].astype(int)\ntrain_labels['correct'] = train_labels.correct.astype(np.int8).values\ngroup = ['session_id', 'question']\nY = pd.pivot_table(train_labels.groupby(group)['correct'].max().reset_index(), \n                   index='session_id', columns='question', \n                   values='correct', aggfunc='max').reset_index()\nY.columns.name = None\nY = Y[['session_id'] + [f'q{i + 1}' for i in range(18)]]\nY = pd.concat([Y, y_test])\nY = Y.reset_index(drop=True)\nY = Y.set_index('session_id').loc[X['session_id'].drop_duplicates()].reset_index()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEV:\n    session_ids = X[X['data_source'] == 'comp']['session_id'].drop_duplicates()[:1000]\n    X = X[X['session_id'].isin(session_ids)].reset_index(drop=True)\n    Y = Y[Y['session_id'].isin(session_ids)].reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Concat event_name & name","metadata":{}},{"cell_type":"code","source":"X['event_name_name'] = X['event_name'] + '_' + X['name']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare","metadata":{}},{"cell_type":"code","source":"X = X[X['level'] < 23]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncomplete_session_ids = Y[Y['q18'] != -1]['session_id']\nif DATA == 'COMP':\n    x_test = X[(X['session_id'].isin(complete_session_ids)) & \n               (X['data_source'] != 'comp')].reset_index(drop=True)\n    y_test = Y[Y['session_id'].isin(x_test['session_id'])].reset_index(drop=True)\n    X = X[X['data_source'] == 'comp'].reset_index(drop=True)\n    Y = Y[Y['session_id'].isin(X['session_id'])].reset_index(drop=True)\nelif DATA == 'COMPLETE_SESSIONS':\n    X = X[X['session_id'].isin(complete_session_ids)].reset_index(drop=True)\n    Y = Y[Y['session_id'].isin(complete_session_ids)].reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Folds","metadata":{}},{"cell_type":"code","source":"COMP_SESSION_IDS = sorted(list(set(X[X['data_source'] == 'comp']['session_id'])))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLDS = [[COMP_SESSION_IDS[i::N_FOLDS] for i in range(N_FOLDS)]]\nif N_BAGS > 1:\n    np.random.seed(SEED)\n    for _ in range(N_BAGS - 1):\n        index = np.random.randint(0, N_FOLDS, len(COMP_SESSION_IDS))\n        FOLDS.append([np.array(COMP_SESSION_IDS)[index == f].tolist() for f in range(N_FOLDS)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Collections","metadata":{}},{"cell_type":"code","source":"if BUILD:\n    activities = {\n        '0-4': ['tunic', 'report', 'plaque'],\n        '5-12': ['businesscards', 'logbook', 'reader', 'wellsbadge', 'journals'],\n        '13-22': ['directory', 'reader_flag', 'journals_flag'],\n    }\n    collections = dict()\n    for level_group in tqdm(LEVEL_GROUPS):\n        x = X[X['level_group'] == level_group]\n        event_names = ['cutscene_click', 'map_click', 'map_hover', 'navigate_click', \n                       'notebook_click', 'notification_click', 'object_click', \n                       'object_hover', 'observation_click', 'person_click']\n        tmp = x['fqid'].dropna().value_counts()\n        fqids = tmp[tmp > len(x) * 0.001].index.tolist()\n        names = x['name'].dropna().drop_duplicates().tolist()\n        event_name_names = X['event_name_name'].value_counts().index.tolist()\n        room_fqids = x['room_fqid'].dropna().drop_duplicates().tolist()\n        texts = x['text'].dropna().drop_duplicates().tolist()\n        text_fqids = x['text_fqid'].dropna().drop_duplicates().tolist()\n        pages = x['page'].dropna().drop_duplicates().tolist()\n        \n        collections[level_group] = {\n            'event_names': event_names,\n            'fqids': fqids,\n            'names': names,\n            'event_name_names': event_name_names,\n            'room_fqids': room_fqids,\n            'texts': texts,\n            'text_fqids': text_fqids,\n            'activities': activities[level_group],\n            'pages': pages,\n        }\n    pickle.dump(collections, open(f'../data/processed/collections_{VERSION}.pkl', 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions and classes","metadata":{}},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def get_levels(level_group):\n    level_group_split = level_group.split('-')\n    level_group_min = int(level_group_split[0])\n    level_group_max = int(level_group_split[1])\n    return [i for i in range(level_group_min, level_group_max + 1)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_questions(level_group):\n    return ([1, 2, 3] if level_group == '0-4' \n            else [4, 5, 6, 7, 8, 9, 10, 11, 12, 13] if level_group == '5-12' \n            else [14, 15, 16, 17, 18])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_level_group(question):\n    if question < 4:\n        return '0-4'\n    elif question < 14:\n        return '5-12'\n    return '13-22'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_feature_name(feature_name):\n    return re.sub('[^A-Za-z0-9_]', '_', str(feature_name))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_features(question):\n    features = pickle.load(open(f\"../models/{VERSION}/features.pkl\", \"rb\"))\n    return features[f\"q{question}\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"def fast_f1_score(labels, preds):\n    all_positives = (preds + labels == 2).mean()\n    all_negatives = (preds + labels == 0).mean()\n    score = 1 - (1 - all_negatives - all_positives) / (1 - (all_negatives - all_positives) ** 2)\n    return score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_questions(preds, labels, questions, thr):\n    thresholds = thr if type(thr) == list else [thr] * 18\n    scores = dict()\n    preds_binarized = preds.copy()\n    for question in questions:\n        preds_binarized[f'q{question}'] = (preds[f'q{question}'].values > thresholds[question - 1]).astype('int')\n        score = fast_f1_score(labels[f'q{question}'].values, preds_binarized[f'q{question}'].values)\n        scores[f'q{question}'] = np.round(score, decimals=5)\n    if len(questions) > 1:\n        score = fast_f1_score(labels.melt().drop('variable', axis=1).values, \n                              preds_binarized.melt().drop('variable', axis=1).values)\n        scores['overall'] = np.round(score, decimals=5)\n    return scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def optimize_threshold(preds, labels, step_size=0.005):\n    thresholds = []\n    best_score = 0\n    best_threshold = 0\n    all_labels = labels.melt().drop('variable', axis=1).values\n    all_preds = preds.melt().drop('variable', axis=1).values    \n    for threshold in np.arange(0.5, 0.81, step_size):\n        binarized_preds = (all_preds > threshold).astype('int')\n        score = fast_f1_score(all_labels, binarized_preds)\n        thresholds.append(threshold)\n        if score > best_score:\n            best_score = score\n            best_threshold = threshold\n    return best_threshold","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def optimize_thresholds(preds, labels, threshold):\n    thrs = []\n    binarized_preds = preds.copy()\n    binarized_preds = 1 * (binarized_preds >= threshold)\n    binarized_preds\n    for q in range(1, 18 + 1):\n        best_score = 0\n        for thr in np.arange(0.5, 0.8, 0.005):\n            oof_preds = binarized_preds.copy()\n            oof_preds[f'q{q}'] = 1 * (preds[[f'q{q}']] >= thr)\n            score = score_questions(oof_preds, labels, range(1, 18 + 1), thr=thr)['overall']\n            if score > best_score:\n                best_score = score\n                best_thr = thr\n        thrs.append(best_thr)\n    return thrs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_score(preds, labels):\n    threshold = np.round(optimize_threshold(preds, labels), 3)\n    scores = score_questions(preds, labels, questions=range(1, 18 + 1), thr=threshold)\n    return scores, threshold","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_test(data, targets, thr, n_bags, n_folds, n_seeds, n_sets=2, verbose=True):\n    models = get_models([q for q in range(1, 18 + 1)], n_bags=N_BAGS, n_folds=N_FOLDS, n_seeds=N_SEEDS)\n    predictions = pd.DataFrame(index=targets.index)\n    for q in range(1, 18 + 1):\n        x, _ = data[f'q{q}']\n        feats = get_features(q)\n        preds = []\n        for b in range(n_bags):\n            for f in range(n_folds):\n                for s in range(n_seeds):\n                    k = f'q{q}b{b}f{f}s{s}'\n                    inputs = x[feats].fillna(-999999).astype(np.float32).values\n                    pred = (models[k].predict_proba(inputs)[:, 1] if MODEL_TYPE != 'LGB' \n                            else models[k].predict(inputs))\n                    preds.append(pred)\n        preds = expit(np.mean(logit(preds), axis=0)) if EXPIT else np.mean(preds, axis=0)\n        predictions[f'q{q}'] = preds\n    score = score_questions(predictions, targets, questions=range(1, 18 + 1), thr=thr)\n    df = pd.DataFrame([score])\n    df.loc[:, [f'q{q}' for q in range(1, 18 + 1)]] = np.round(df.loc[:, [f'q{q}' for q in range(1, 18 + 1)]], 3)\n    return score, df, predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_importances(features, question, n_bags, n_folds, n_seeds):\n    models = get_models([question], n_bags, n_folds, n_seeds)\n    importances = pd.DataFrame()\n    importances['feature'] = features\n    questions = []\n    for b in range(n_bags):\n        for s in range(n_seeds):\n            for f in range(n_folds):\n                name = f'q{question}b{b}f{f}s{s}'\n                questions.append(name)\n                if MODEL_TYPE == 'XGB':\n                    importances[name] = models[name].feature_importances_\n                elif MODEL_TYPE == 'LGB':\n                    importances[name] = models[name].feature_importance(importance_type='split')\n                elif MODEL_TYPE == 'CATBOOST':\n                    importances[name] = models[name].get_feature_importance()\n    importances[f'q{question}'] = importances[questions].mean(axis=1)\n    \n    n_features = len(importances['feature'])\n    importances_plot = plt.figure(figsize=(6, n_features // 4))\n    index = importances.sort_values(f'q{question}').index.tolist()\n    plt.barh(importances['feature'][index][-n_features:], \n             importances[f'q{question}'][index][-n_features:], alpha=0.25)\n    plt.margins(y=0)\n    plt.box(False)\n    plt.close(importances_plot)\n    return {'df': importances, 'plot': importances_plot}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def explain(features, questions, n_bags, n_folds, n_seeds):\n    importances = dict()\n    for question in questions:\n        importances[f'q{question}'] = get_importances(features, question, n_bags, n_folds, n_seeds)\n    return importances","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"def get_models(questions, n_bags, n_folds, n_seeds):\n    models = dict()\n    for q in questions:\n        for b in range(n_bags):\n            for f in range(n_folds):\n                for s in range(n_seeds):\n                    model_name = f'q{q}b{b}f{f}s{s}'\n                    if MODEL_TYPE == 'XGB':\n                        model = XGBClassifier(n_jobs=8)\n                        model.load_model(f'../models/{VERSION}/{model_name}.xgb')\n                    elif MODEL_TYPE == 'LGB':\n                        model = pickle.load(open(f'../models/{VERSION}/{model_name}.lgb', 'rb'))\n                    elif MODEL_TYPE == 'CATBOOST':\n                        model = CatBoostClassifier()\n                        model.load_model(f'../models/{VERSION}/{model_name}.cbm')\n                    models[model_name] = model\n    return models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_xgb_model(x_train, y_train, x_val, y_val, model_name, seed, verbose=0):\n    xgb_params = {\n        'booster': 'gbtree',\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'learning_rate': 0.02,\n        'max_depth': 4,\n        'alpha': 4,\n        'n_estimators': 10000,\n        'early_stopping_rounds': 100,\n        'tree_method': 'gpu_hist',\n        'subsample': 0.8,\n        'colsample_bytree': 0.2,\n        'use_label_encoder': False,\n        'n_jobs': 8,\n        'seed': seed,\n    }           \n    model = XGBClassifier(**xgb_params)\n    model.fit(\n        x_train, \n        y_train,\n        eval_set=[(x_train, y_train), (x_val, y_val)],\n        verbose=20 if verbose >= 2 else 0,\n    )       \n    model.save_model(f'../models/{VERSION}/{model_name}.xgb')    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_lgb_model(x_train, y_train, x_val, y_val, model_name, seed, verbose=0):        \n    params = {\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.01,\n        'num_leaves': 2 ** 4,\n        'min_data_in_leaf': 50,\n        'max_depth': 5,\n        'colsample_bytree': 0.2,\n        'linear_lambda': 1,\n        'verbose': -1,\n        'seed': seed,\n        'n_jobs': 8,\n    }\n    \n    lgb_train = lgb.Dataset(x_train, label=y_train)\n    lgb_val = lgb.Dataset(x_val, label=y_val)\n\n    early_stopping_callback = lgb.early_stopping(100, first_metric_only=False, verbose=False)\n    verbose_callback = lgb.log_evaluation(0)\n\n    model = lgb.train(\n        params,\n        lgb_train,\n        valid_sets=[lgb_train, lgb_val],\n        num_boost_round=10000,\n        callbacks=[early_stopping_callback, verbose_callback]\n    )\n\n    pickle.dump(model, open(f'../models/{VERSION}/{model_name}.lgb', 'wb'))\n    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_catboost_model(x_train, y_train, x_val, y_val, model_name, seed, verbose=0):\n    train_pool = Pool(x_train.astype(np.float32), y_train)\n    val_pool = Pool(x_val.astype(np.float32), y_val)\n    \n    model = CatBoostClassifier(\n        iterations=10000,\n        early_stopping_rounds=100,\n        depth=4,\n        learning_rate=0.05,\n        loss_function='Logloss',\n        subsample=0.8,\n        colsample_bylevel=0.0,\n        verbose=0,\n        random_seed=seed,\n        thread_count=12,\n    )\n    model = model.fit(train_pool, eval_set=val_pool)   \n    model.save_model(f'../models/{VERSION}/{model_name}.cbm')\n    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_gbdt(x_train, y_train, x_val, y_val, model_name, model_type, seed, verbose=0):        \n    if model_type == 'XGB':\n        model = train_xgb_model(x_train, y_train, x_val, y_val, model_name, seed, verbose)\n    elif model_type == 'LGB':\n        model = train_lgb_model(x_train, y_train, x_val, y_val, model_name, seed, verbose)\n    elif model_type == 'CATBOOST':\n        model = train_catboost_model(x_train, y_train, x_val, y_val, model_name, seed, verbose)\n    oof_predictions = (model.predict_proba(x_val)[:, 1] if model_type != 'LGB'\n                       else model.predict(x_val))\n    return oof_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"class AggsBuilder:\n    def __init__(self):\n        self.collections = pickle.load(open(f'../data/processed/collections_{VERSION}.pkl', 'rb'))\n        self.aggs = []\n        \n    def doc(self):\n        to_exclude = ['collections', 'aggs']\n        return [el for el in dir(self) if '__' not in el and el not in to_exclude]\n        \n    def collect(self):\n        aggs = self.aggs\n        self.aggs = []\n        return aggs\n    \n    def clear(self):\n        self.aggs = []\n        \n    def add(self, *aggregations):\n        self.aggs.extend([*aggregations])\n        \n    def add_durations(self, level_group):\n        fqids = self.collections[level_group]['fqids']\n        room_fqids = self.collections[level_group]['room_fqids']\n        event_name_names = self.collections[level_group]['event_name_names']\n        texts = self.collections[level_group]['texts']\n        levels = get_levels(level_group)\n        activities = self.collections[level_group]['activities']\n        pages = self.collections[level_group]['pages']\n        \n        text_root = pl.col('duration').filter(~pl.col('text').is_null())\n        \n        self.add(pl.col('duration').drop_nulls().sum()\n                 .alias(f'level_group_{level_group}_duration_sum'))\n        \n        self.add(*[pl.col('duration').filter(pl.col('room_fqid') == r).sum()\n                   .alias(f'level_group_{level_group}_room_fqid_{r}_duration_sum') for r in room_fqids])\n        \n        self.add(*[pl.col('duration').filter(pl.col('fqid') == f).sum()\n                   .alias(f'level_group_{level_group}_fqid_{f}_duration_sum') for f in fqids])\n        \n        self.add(*[pl.col('duration').filter(pl.col('text') == t).sum()\n                   .alias(f'level_group_{level_group}_text_{t}_duration_sum') for t in texts])\n        \n        self.add(*[pl.col('duration').filter(pl.col('fqid').str.contains(f)).sum()\n                   .alias(f'level_group_{level_group}_activity_{f}_duration_sum') for f in activities])\n        \n        self.add(*[pl.col('hover_duration').filter(pl.col('fqid').str.contains(f)).sum()\n                   .alias(f'level_group_{level_group}_fqid_{f}_hover_duration_sum') for f in fqids])\n\n        self.add(*[pl.col('duration').filter(pl.col('event_name_name') == e).sum()\n                   .alias(f'level_group_{level_group}_event_name_name_{e}_duration_sum') \n                   for e in event_name_names])\n\n        self.add(*[pl.col('duration').filter((pl.col('level') == l) & (pl.col('event_name_name') == e)).sum()\n                   .alias(f'level_{l}_event_name_name_{e}_duration_sum') \n                   for e in event_name_names for l in levels])\n\n        self.add(*[pl.col('duration').filter((pl.col('room_fqid') == r) & (pl.col('event_name_name') == e)).sum()\n                   .alias(f'level_group_{level_group}_room_fqid_{r}_event_name_name_{e}_duration_sum') \n                   for e in event_name_names for r in room_fqids])\n        \n        self.add(*[(pl.col('elapsed_time').filter(pl.col('fqid').str.contains(f)).max() - \n                    pl.col('elapsed_time').filter(pl.col('fqid').str.contains(f)).min())\n                   .alias(f'level_group_{level_group}_activity_{f}_elapsed_time_diff') for f in activities])\n        \n        self.add(*[pl.col('prev_duration').filter(pl.col('fqid') == f).sum()\n                   .alias(f'level_group_{level_group}_fqid_{f}_prev_duration_sum') for f in fqids])\n        \n        self.add(*[pl.col('prev_duration').filter(pl.col('text') == t).sum()\n                   .alias(f'level_group_{level_group}_text_{t}_prev_duration_sum') for t in texts])\n        \n        self.add(*[pl.col('prev_duration').filter(pl.col('fqid').str.contains(f)).sum()\n                   .alias(f'level_group_{level_group}_activity_{f}_prev_duration_sum') for f in activities])\n\n        return self\n    \n    def add_counts(self, level_group):\n        fqids = self.collections[level_group]['fqids']\n        room_fqids = self.collections[level_group]['room_fqids']\n        event_name_names = self.collections[level_group]['event_name_names']\n        texts = self.collections[level_group]['texts']\n        levels = get_levels(level_group)\n        activities = self.collections[level_group]['activities']\n        pages = self.collections[level_group]['pages']\n        \n        self.add(pl.col('index').count()\n                 .alias(f'level_group_{level_group}_cnt'))\n        \n        self.add(*[pl.col('index').filter(pl.col('room_fqid') == r).count()\n                   .alias(f'level_group_{level_group}_room_fqid_{r}_cnt') for r in room_fqids])\n        \n        self.add(*[pl.col('index').filter(pl.col('fqid').str.contains(f)).count()\n                   .alias(f'level_group_{level_group}_activity_{f}_cnt') for f in activities])\n        \n        self.add(*[pl.col('index').filter(pl.col('level') == l).count()\n                   .alias(f'level_{l}_cnt') for l in levels])\n        \n        self.add(*[pl.col('index').filter(pl.col('fqid') == f).count()\n                   .alias(f'level_group_{level_group}_fqid_{f}_cnt') for f in fqids])\n        \n        self.add(*[pl.col('index').filter(pl.col('text_fqid') == t).count()\n                   .alias(f'level_group_{level_group}_text_fqid_{t}_cnt') for t in text_fqids])\n        \n        self.add(*[pl.col('index').filter(pl.col('page') == p).count()\n                   .alias(f'level_group_{level_group}_page_{p}_cnt') for p in pages])\n        \n        self.add(*[pl.col('index').filter(pl.col('event_name_name') == e).count()\n                   .alias(f'level_group_{level_group}_event_name_name_{e}_cnt') for e in event_name_names])\n        \n        self.add(*[pl.col('index').filter((pl.col('event_name_name') == e) & (pl.col('level') == l)).count()\n                   .alias(f'level_group_{level_group}_level_{l}_event_name_name_{e}_cnt') \n                   for e in event_name_names for l in levels])\n        \n        self.add(*[pl.col('index').filter((pl.col('event_name_name') == e) & (pl.col('room_fqid') == r)).count()\n                   .alias(f'level_group_{level_group}_room_fqid_{r}_event_name_name_{e}_cnt') \n                   for e in event_name_names for r in room_fqids])\n        \n        return self\n    \n    def add_mouse(self, level_group):\n        filter_condition = (pl.col('event_name') == 'object_click') & (pl.col('name') == 'basic')\n        activities = self.collections[level_group]['activities']\n        \n        self.add(*[pl.col('room_coor_x').filter(filter_condition & pl.col('fqid').str.contains(f)).mean()\n                   .alias(f'level_group_{level_group}_{f}_room_coor_x_mean') for f in activities])\n        self.add(*[pl.col('room_coor_x').filter(filter_condition & pl.col('fqid').str.contains(f)).std()\n                   .alias(f'level_group_{level_group}_{f}_room_coor_x_std') for f in activities])\n        self.add(*[pl.col('room_coor_x').filter(filter_condition & pl.col('fqid').str.contains(f)).mean()\n                   .alias(f'level_group_{level_group}_{f}_room_coor_y_mean') for f in activities])\n        self.add(*[pl.col('room_coor_x').filter(filter_condition & pl.col('fqid').str.contains(f)).std()\n                   .alias(f'level_group_{level_group}_{f}_room_coor_y_std') for f in activities])\n        \n        return self\n        \n    def add_notebook(self, level_group):\n        self.add(*[pl.col('duration').filter((pl.col('level') == l) & \n                                             ((pl.col('event_name') == 'notebook_click') & \n                                              (pl.col('name') != 'close'))).sum()\n                   .alias(f'level_{l}_notebook_duration_sum') for l in get_levels(level_group)])\n        return self\n\n    def add_globals(self, level_group):\n        return self","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Data:\n    def __init__(self, x, y, mode='train'):\n        self.x = x\n        self.y = y\n        self.session_ids = {\n            '0-4': y[y['q13'] != -1]['session_id'].tolist(),\n            '5-12': y[y['q13'] != -1]['session_id'].tolist(),\n            '13-22': y[y['q18'] != -1]['session_id'].tolist(),\n        }\n        self.mode = mode\n        self.aggs_builder = AggsBuilder()\n        self.level_group_data = {}\n        self.engineer()\n        \n    def engineer(self):\n        for level_group in LEVEL_GROUPS:\n            x = self.x[self.x['level_group'] == level_group]\n            y = self.y.copy()\n            x['session_id'] = x['session_id'].astype(int)\n            x_lg = self.engineer_level_group_features(x, level_group)\n            if self.mode == 'train':\n                x_lg = self.delete_features(x_lg)\n            x_lg.columns = [clean_feature_name(c) for c in x_lg.columns]\n            self.level_group_data[level_group] = (x_lg, y)\n            print(level_group, len(x_lg.columns))\n            \n    def __getitem__(self, question_str):\n        question = int(question_str[1:])\n        if question < 4:\n            x = self.level_group_data['0-4'][0]\n            x = x[x['session_id'].isin(self.session_ids['0-4'])].reset_index(drop=True)\n        elif question < 14:\n            x = self.level_group_data['0-4'][0].merge(\n                self.level_group_data['5-12'][0], on='session_id', how='left')\n            x = x[x['session_id'].isin(self.session_ids['5-12'])].reset_index(drop=True)\n        else:\n            x = self.level_group_data['0-4'][0].merge(\n                self.level_group_data['5-12'][0], on='session_id', how='left')\n            x = x.merge(self.level_group_data['13-22'][0], on='session_id', how='left')\n            x = x[x['session_id'].isin(self.session_ids['13-22'])].reset_index(drop=True)\n        y = self.y[self.y['session_id'].isin(x['session_id']) & \n                   (self.y[question_str] > -1)][['session_id', question_str]].set_index('session_id')\n        x = x[x['session_id'].isin(y.index)].reset_index(drop=True)\n        return x, y\n        \n    def engineer_level_group_features(self, x, level_group):\n        columns = [((pl.col('elapsed_time').shift(-1) - pl.col('elapsed_time')).fill_null(0)\n                    .over(['session_id', 'level_group'])\n                    .alias('duration')),\n                   ((pl.col('elapsed_time').shift(1) - pl.col('elapsed_time')).fill_null(0)\n                    .over(['session_id', 'level_group'])\n                    .alias('prev_duration'))]\n        aggs = (self.aggs_builder\n                .add_durations(level_group)\n                .add_counts(level_group)\n                .add_mouse(level_group)\n                .add_notebook(level_group)\n                .collect())\n        return (pl.from_pandas(x)\n                .lazy()\n                .drop(['fullscreen', 'hq', 'music'])\n                .with_columns(columns)\n                .groupby(['session_id'], maintain_order=True)\n                .agg(aggs)\n                .sort(['session_id'])\n                .collect()\n                .to_pandas())\n    \n    def delete_features(self, x):\n        col_to_delete = []\n        for c in x.columns[1:]:\n            if len(set(x[c].fillna(-999999))) == 1:\n                col_to_delete.append(c)\n            else:\n                value_counts = x[c].value_counts()\n                if value_counts.sum() < 10:\n                    col_to_delete.append(c)\n        x.drop(col_to_delete, axis=1, inplace=True)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train(data, targets, bags, n_seeds, model_type=MODEL_TYPE, verbose=0):\n    features = {}\n    oof_predictions = targets[[]].copy()\n    outputs = []\n    for b, folds in enumerate(bags):\n        results = []\n        for f, session_ids in enumerate(folds):\n            for q in tqdm(range(1, 18 + 1)):\n                x, y = data[f'q{q}']\n                y = y.reset_index()\n                features[f'q{q}'] = x.columns[1:].tolist()\n                x_train = x[~x['session_id'].isin(session_ids)][features[f'q{q}']]\n                x_train = x_train.fillna(-999999).astype(np.float32).values\n                y_train = y[~y['session_id'].isin(session_ids)][f'q{q}'].values\n                x_val = x[x['session_id'].isin(session_ids)][features[f'q{q}']]\n                x_val = x_val.fillna(-999999).astype(np.float32).values\n                y_val = y[y['session_id'].isin(session_ids)][f'q{q}'].values\n                for s in range(n_seeds):\n                    preds = train_gbdt(x_train, y_train, x_val, y_val, \n                                       f'q{q}b{b}f{f}s{s}', model_type, seed=s, verbose=verbose)\n                    oof_predictions.loc[session_ids, f'q{q}b{b}s{s}'] = preds\n                preds = oof_predictions[[f'q{q}b{b}s{s}' for s in range(n_seeds)]]\n                oof_predictions[f'q{q}b{b}'] = (expit(np.mean(logit(preds), axis=1)) if EXPIT \n                                                else np.mean(preds, axis=1))\n            oof_preds = oof_predictions[[f'q{q}b{b}' for q in range(1, 18 + 1)]].loc[session_ids]\n            oof_preds.columns = [f'q{q}' for q in range(1, 18 + 1)]\n            y_oof = targets.loc[session_ids]\n            threshold = optimize_threshold(oof_preds, y_oof, step_size=0.001)\n            result = score_questions(oof_preds, y_oof, questions=range(1, 18 + 1), thr=threshold)\n            for q in range(1, 18 + 1):\n                result[f'q{q}'] = np.round(result[f'q{q}'], 3)\n            result['thr'] = np.round(threshold, 3)\n            results.append(result)\n            clear_output()\n            for df in outputs:\n                display(df)\n            display(pd.DataFrame(results, index=[f'b{b}f{f}' for f in range(len(results))]))\n        session_ids = oof_predictions[oof_predictions[f'q1b{b}'].notnull()].index.tolist()\n        oof_preds = oof_predictions[[f'q{q}b{b}' for q in range(1, 18 + 1)]].loc[session_ids]\n        oof_preds.columns = [f'q{q}' for q in range(1, 18 + 1)]\n        threshold = optimize_threshold(oof_preds, targets.loc[session_ids], step_size=0.001)\n        result = score_questions(oof_preds, targets.loc[session_ids], questions=range(1, 18 + 1), thr=threshold)\n        result['thr'] = np.round(threshold, 3)\n        for q in range(1, 18 + 1):\n            result[f'q{q}'] = np.round(result[f'q{q}'], 3)\n        results.append(result)\n        index = [f'b{b}f{f}' for f in range(len(folds))] + [f'b{b}']\n        outputs.append(pd.DataFrame(results, index=index))\n        clear_output()\n        for df in outputs:\n            display(df)\n    for q in range(1, 18 + 1):\n        preds = oof_predictions[[f'q{q}b{b}s{s}' for s in range(n_seeds) for b in range(N_BAGS)]]\n        oof_predictions[f'q{q}'] = expit(np.mean(logit(preds), axis=1)) if EXPIT else np.mean(preds, axis=1)\n    return oof_predictions, features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! rm -rf ../models/\"$VERSION\"\n! mkdir ../models/\"$VERSION\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_data = Data(X, Y, mode='train')\ntargets = Y[Y['session_id'].isin(COMP_SESSION_IDS)].set_index('session_id')\noof_predictions, features = train(train_data, targets, bags=FOLDS, n_seeds=N_SEEDS)\nimportances = {f'q{q}': explain(features[f'q{q}'], [q], n_bags=N_BAGS, n_folds=N_FOLDS, n_seeds=N_SEEDS)[f'q{q}'] \n               for q in range(1, 18 + 1)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(features, open(f'../models/{VERSION}/features.pkl', 'wb'))\nlen(features['q1']), len(features['q4']), len(features['q14'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scoring","metadata":{}},{"cell_type":"code","source":"oof_preds = oof_predictions[[f'q{q}' for q in range(1, 18 + 1)]]\ny_oof = Y[Y['session_id'].isin(oof_predictions.index)].set_index('session_id')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(oof_preds, open(f'../models/{VERSION}/oof_preds.pkl', 'wb'))\npickle.dump(y_oof, open(f'../models/{VERSION}/y_oof.pkl', 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif DATA == 'COMP':\n    test_data = Data(x_test, y_test, mode='test')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1 thr","metadata":{}},{"cell_type":"code","source":"threshold = optimize_threshold(oof_preds, y_oof, step_size=0.001)\nnp.round(threshold, 3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame([score_questions(oof_preds, y_oof, questions=range(1, 18 + 1), thr=threshold)])\ndf.loc[:, [f'q{q}' for q in range(1, 18 + 1)]] = np.round(df.loc[:, [f'q{q}' for q in range(1, 18 + 1)]], 3)\ndisplay(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not DEV and DATA == 'COMP':\n    score, df, predictions = evaluate_test(\n        test_data, y_test.set_index('session_id'), thr=threshold, n_bags=N_BAGS, n_folds=N_FOLDS, n_seeds=N_SEEDS)\n    display(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = pd.read_parquet('../data/processed/x_test.parquet')\nx['event_name_name'] = x['event_name'] + '_' + x['name']\ny = pd.read_parquet('../data/processed/y_test.parquet')\ny = y[y['q18'] > -1].reset_index(drop=True)\nx = x[x['session_id'].isin(y['session_id'])].reset_index(drop=True)\nx = x.sort_values(['session_id', 'index']).reset_index(drop=True)\ny = y.set_index('session_id').loc[x['session_id'].drop_duplicates()].reset_index()\ntest_data_202211 = Data(x, y, mode='test')\nscore, df, predictions = evaluate_test(\n    test_data_202211, y.set_index('session_id'), thr=0.62, n_bags=N_BAGS, n_folds=N_FOLDS, n_seeds=N_SEEDS)\ndisplay(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1 thr per question","metadata":{}},{"cell_type":"code","source":"thrs = optimize_thresholds(oof_preds, y_oof, threshold)\npickle.dump(thrs, open(f'../models/{VERSION}/thresholds.pkl', 'wb'))\nprint([np.round(t, 3) for t in thrs])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame([score_questions(oof_preds, y_oof, questions=range(1, 18 + 1), thr=thrs)])\ndf.loc[:, [f'q{q}' for q in range(1, 18 + 1)]] = np.round(df.loc[:, [f'q{q}' for q in range(1, 18 + 1)]], 3)\ndisplay(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not DEV and DATA == 'COMP':\n    score, df, predictions = evaluate_test(\n        test_data, y_test.set_index('session_id'), thr=thrs, n_bags=N_BAGS, n_folds=N_FOLDS, n_seeds=N_SEEDS)\n    display(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Overall","metadata":{}},{"cell_type":"code","source":"def score_all(predictions, targets, n_bags, n_thresholds=1):\n    results = []\n    for b in range(n_bags):\n        oof_preds = predictions[[f'q{q}b{b}' for q in range(1, 18 + 1)]]\n        oof_preds.columns = [f'q{q}' for q in range(1, 18 + 1)]\n        thr = optimize_threshold(oof_preds, targets, step_size=0.001)\n        if n_thresholds > 1:\n            thr = optimize_thresholds(oof_preds, targets, thr)\n        result = score_questions(oof_preds, targets, questions=range(1, 18 + 1), thr=thr)\n        if n_thresholds == 1:\n            result['thr'] = np.round(thr, 3)\n        for q in range(1, 18 + 1):\n            result[f'q{q}'] = np.round(result[f'q{q}'], 3)\n        results.append(result)\n    oof_preds = predictions[[f'q{q}' for q in range(1, 18 + 1)]]\n    thr = optimize_threshold(oof_preds, targets, step_size=0.001)\n    if n_thresholds > 1:\n        thr = optimize_thresholds(oof_preds, targets, thr)\n    result = score_questions(oof_preds, targets, questions=range(1, 18 + 1), thr=thr)\n    if n_thresholds == 1:\n        result['thr'] = np.round(thr, 3)\n    for q in range(1, 18 + 1):\n        result[f'q{q}'] = np.round(result[f'q{q}'], 3)\n    results.append(result)\n    index = [f'b{b}' for b in range(N_BAGS)] + [f'overall']\n    display(pd.DataFrame(results, index=index))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_all(oof_predictions, targets, N_BAGS)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_all(oof_predictions, targets, N_BAGS, n_thresholds=18)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bags and seeds","metadata":{}},{"cell_type":"code","source":"def score_subparts(predictions, targets, folds, bags, seeds, n_folds=N_FOLDS, n_thresholds=1):\n    preds = pd.DataFrame()\n    for q in range(1, 18 + 1):\n        pred = predictions[[f'q{q}b{b}s{s}' for s in seeds for b in bags]]\n        preds[f'q{q}'] = expit(np.mean(logit(pred), axis=1)) if EXPIT else np.mean(pred, axis=1)\n    thr = np.round(optimize_threshold(preds, targets, step_size=0.001), 3)\n    if n_thresholds > 1:\n        thr = optimize_thresholds(preds, targets, thr)\n    return score_questions(preds, targets, questions=range(1, 18 + 1), thr=thr)['overall'], np.round(thr, 3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_subparts(oof_predictions, targets, folds=FOLDS, bags=range(N_BAGS), seeds=range(N_SEEDS), n_thresholds=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nscore_subparts(oof_predictions, targets, folds=FOLDS, bags=range(N_BAGS), seeds=range(N_SEEDS), n_thresholds=18)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explanation","metadata":{}},{"cell_type":"code","source":"importances['q1']['df'].sort_values('q1', ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances['q1']['df'].sort_values('q1', ascending=False).tail(50)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances['q8']['df'][importances['q8']['df']['q8'] == 0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances['q15']['df'][importances['q15']['df']['q15'] == 0]","metadata":{},"execution_count":null,"outputs":[]}]}